# Job Scheduling with Multiple Machines

The Julia implementation provided here solves the **parallel machine scheduling problem (P||Cmax)** using a metaheuristic based on **simulated annealing**.

## Parameters
- A set of jobs \(J = \{1,2,\dots,n\}\) with corresponding processing times \(p_j\).
- A set of machines \(M = \{1,2,\dots,k\}\).

## Goal
Find an assignment of jobs to machines that minimizes the **makespan**, i.e. the maximum completion time across all machines.

## Usage
Run from the command line: `julia scheduling.jl <instance-file> <time-limit-seconds> <seed>`

## Solution Representation
The algorithm works with an **indirect solution representation** in the form of a **permutation of all jobs**.  
- Each permutation is decoded into an actual schedule using **list scheduling**: jobs are placed one after another on the machine with the currently least load.  
- This guarantees that every permutation corresponds to a **valid solution**.  
- Because list scheduling is a greedy procedure, the decoded schedules are already relatively good.  
- Importantly, there always exists an **optimal solution** that can be represented in this way.

## Neighborhood Structure
- The neighborhood is defined by **swap moves** on the permutation (exchanging two jobs).  
- This ensures that the neighborhood is **connected**: any permutation can be reached from any other by a sequence of swaps.  
- Neighbor solutions are always valid, so the search can intensively explore large parts of the solution space.  
- To diversify, after a large number of non‑improving iterations, a random partial shuffle of the permutation is applied.

## Algorithm Outline
1. **Initial solution**: Jobs are sorted in decreasing order of processing times and decoded via list scheduling.  
   - This yields a **4/3‑approximation guarantee** for the makespan.  
2. **Main loop**:  
   - Neighbor solutions are generated by swapping jobs in the permutation.  
   - A **simulated annealing acceptance criterion** allows occasional acceptance of worse solutions to escape local minima.  
   - A **dynamic geometric cooling schedule** adapts the temperature so that a target final temperature (scaled by the objective value) is reached within the time limit.  
   - After many non‑improving iterations, a **random partial shuffle** diversifies the search.  
3. **Termination**: The best solution found within the given time limit is returned.



